import numpy as np                    #libraries 
import matplotlib.pyplot as plt       # "as" is the alias 
from sklearn.linear_model import LinearRegression  
from sklearn.metrics import mean_squared_error, r2_score

#create artificial dataset for practice 
x = np.random.random(size = 100) # array x
l =len(x)
c = np.random.uniform(low =0.11, high=  0.12, size = 100) # constant term 
m = 0.1                          # slope  
y = m*x + c                      # generate the new line

#linear regression using MSE
#plt.scatter(x,y, c = "blue")     # plot  
#x = x.reshape(-1,1)
#reg = LinearRegression().fit(x, y) #linear regression  
#c_pred = reg.intercept_
#m_pred = reg.coef_
##plt.plot(diabetes_X_test, diabetes_y_pred, color='blue', linewidth=3)
#plt.plot(x, m_pred*x + c_pred, linestyle='solid',color = 'black')

#minibatch_gradient descent
from sklearn.model_selection import train_test_split
x_train,x_test,y_train,y_test=train_test_split(x,y,random_state=6)

class mini_batch_gradient_descent:
    
    def create_batch(self,x_train,y_train,batch_size):
        mini_batches=[]
        data=np.stack((x_train,y_train),axis=1)
        np.random.shuffle(data)
        no_of_batches=x_train.shape[0]//batch_size
        for i in range(no_of_batches):
            mini_batch=data[i*batch_size:(i+1)*batch_size]
            mini_batches.append((mini_batch[:,0],mini_batch[:,1]))
        if x_train.shape[0]%batch_size!=0:
            mini_batch=data[(i+1)*batch_size:]
            mini_batches.append((mini_batch[:,0],mini_batch[:,1]))
        return mini_batches

    def fit(self,x_train,y_train,alpha,epochs,batch_size):
        self.m=np.random.randn(1,1)
        self.c=np.random.randn(1,1)
        l=len(x_train)
        for i in range(epochs):
            batches=self.create_batch(x_train,y_train,batch_size)
            for batch in batches:
                xb=batch[0]
                yb=batch[1]
                xb=xb.reshape(1,xb.shape[0])
                intecept=np.sum((np.dot(self.m,xb)+self.c)-yb)
                slope=np.sum(((np.dot(self.m,xb)+self.c)-yb)*xb)
                self.m=self.m-alpha*(slope/l)
                self.c=self.c-alpha*(intecept/l)
    
    def slope_intercept(self):
        print(f"slope is {self.m[0][0]}")
        print(f"intecept is {self.c[0][0]}")
    
    def predict(self,x_test):
        x_test=x_test.reshape(x_test.shape[0],1)
        self.m=self.m.reshape(self.m.shape[1],self.m.shape[0])
        result=np.dot(x_test,self.m)+self.c
        return result  
  
mgd=mini_batch_gradient_descent()
mgd.fit(x_train,y_train,0.01,4000,4)
mgd.slope_intercept()
y_pred=mgd.predict(x_test)
plt.plot(x_test,y_pred,marker='o',
         color='blue',markerfacecolor='red',
         markersize=10,linestyle='dashed')
plt.scatter(x,y,marker='o',color='red')
